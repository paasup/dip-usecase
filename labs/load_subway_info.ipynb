{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75fbcc82-3c93-421b-8574-ce2be1472ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.functions import lit, substring, col\n",
    "from delta.tables import DeltaTable \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2 \n",
    "import boto3\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "572414db-c408-47c2-a9cf-3d662ded77fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session(app_name,  notebook_name, notebook_namespace, s3_access_key, s3_secret_key, s3_endpoint):\n",
    "    \"\"\"\n",
    "    spark 세션을 creat\n",
    "    \"\"\"\n",
    "    conf = SparkConf()\n",
    "    # Spark driver, executor 설정\n",
    "    conf.set(\"spark.submit.deployMode\", \"client\")\n",
    "    conf.set(\"spark.executor.instances\", \"1\")\n",
    "    conf.set(\"spark.executor.memory\", \"1G\")\n",
    "    conf.set(\"spark.driver.memory\", \"1G\")\n",
    "    conf.set(\"spark.executor.cores\", \"1\")\n",
    "    conf.set(\"spark.kubernetes.namespace\", notebook_namespace)\n",
    "    conf.set(\"spark.kubernetes.container.image\", \"paasup/spark:3.5.2-java17-python3.11-2\")\n",
    "    conf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"default-editor\")\n",
    "    conf.set(\"spark.kubernetes.driver.pod.name\", os.environ[\"HOSTNAME\"])\n",
    "    conf.set(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    conf.set(\"spark.driver.host\", notebook_name+ \"-headless.\" + notebook_namespace + \".svc.cluster.local\")\n",
    "    conf.set(\"spark.driver.port\", \"51810\")        \n",
    "    conf.set(\"spark.broadcast.port\", \"51811\")     \n",
    "    conf.set(\"spark.blockManager.port\", \"51812\")\n",
    "\n",
    "    # s3, delta, postgresql 사용 시 필요한 jar 패키지 설정\n",
    "    jar_list = \",\".join([\n",
    "    \"org.apache.hadoop:hadoop-common:3.3.4\",\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "    \"com.amazonaws:aws-java-sdk:1.11.655\", \n",
    "    \"io.delta:delta-spark_2.12:3.3.1\", # Keep if you might use Delta Lake for other operations\n",
    "    \"org.postgresql:postgresql:42.7.2\" # Added for PostgreSQL\n",
    "    ])\n",
    "    conf.set(\"spark.jars.packages\", jar_list)  \n",
    "\n",
    "    # s3 세팅\n",
    "    conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    conf.set('spark.hadoop.fs.s3a.path.style.access', 'true')\n",
    "    conf.set('spark.hadoop.fs.s3a.connection.ssl.enabled', 'true')\n",
    "    conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    conf.set('spark.hadoop.fs.s3a.access.key', s3_access_key)\n",
    "    conf.set('spark.hadoop.fs.s3a.secret.key', s3_secret_key)\n",
    "    conf.set('spark.hadoop.fs.s3a.endpoint', s3_endpoint)\n",
    "    \n",
    "    ### ssl 검증 비활성화\n",
    "    conf.set(\"spark.driver.extraJavaOptions\", \"-Dcom.amazonaws.sdk.disableCertChecking=true\")\n",
    "    conf.set(\"spark.executor.extraJavaOptions\", \"-Dcom.amazonaws.sdk.disableCertChecking=true\")\n",
    "\n",
    "    # deltalake 세팅\n",
    "    conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
    "\n",
    "    # --- SparkSession 빌드 ---\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(conf=conf) \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b9bda6f-67f0-491e-a7fe-eb3dd21345e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_s3_file(s3_config, source_key, dest_key):\n",
    "    \"\"\"\n",
    "    boto3를 사용하여 S3 내에서 파일을 이동(복사 후 삭제)합니다.\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=s3_config['access_key'],\n",
    "        aws_secret_access_key=s3_config['secret_key'],\n",
    "        endpoint_url=s3_config['endpoint'],\n",
    "        verify=False # 자체 서명 인증서 사용 시 필요할 수 있음\n",
    "    )\n",
    "    try:\n",
    "        copy_source = {'Bucket': s3_config['bucket'], 'Key': source_key}\n",
    "        print(f\"Copying S3 object from {source_key} to {dest_key}...\")\n",
    "        s3_client.copy_object(CopySource=copy_source, Bucket=s3_config['bucket'], Key=dest_key)\n",
    "        print(f\"Deleting source S3 object: {source_key}...\")\n",
    "        s3_client.delete_object(Bucket=s3_config['bucket'], Key=source_key)\n",
    "        print(f\"Successfully moved {source_key} to {dest_key}.\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"Error moving S3 file: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during S3 move: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d809b0e-7443-4377-ae62-ae9c50d4a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subway_info_overwrite(spark, s3_config, pg_config): \n",
    "    \"\"\"\n",
    "    CSV--> DELTA --> POSTGRESQL 처리하는 메인 파이프라인\n",
    "    \"\"\"\n",
    "    pg_url = f\"jdbc:postgresql://{pg_config['host']}:{pg_config['port']}/{pg_config['dbname']}\"\n",
    "    pg_properties = {\n",
    "        \"user\": pg_config['user'],\n",
    "        \"password\": pg_config['password'],\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "    print(f\" bucket name is {s3_config['bucket']}\")\n",
    "    csv_source_key = f\"datasource/서울교통공사_노선별 지하철역 정보.csv\"\n",
    "    csv_archive_key = f\"archive/서울교통공사_노선별 지하철역 정보.csv\"\n",
    "    csv_s3_path = f\"s3a://{s3_config['bucket']}/{csv_source_key}\"\n",
    "    delta_s3_path = f\"s3a://{s3_config['bucket']}/{s3_config['delta_path']}\"\n",
    "    \n",
    "    try:    \n",
    "        print(f\"\\n--- Starting processing for {pg_config['table']} ---\")\n",
    "\n",
    "        # --- 단계 1: S3 CSV 읽기 ---\n",
    "        print(f\"Reading CSV from {csv_s3_path}...\")\n",
    "        # CSV 스키마나 옵션(overwrite, inferSchema...)은 소스 파일에 맞게 조정\n",
    "        csv_df = spark.read.option(\"header\", \"true\").option(\"overwriteSchema\", \"true\").csv(csv_s3_path)\n",
    "        csv_df.createOrReplaceTempView(\"subway_info\")\n",
    "        csv_df.show(5)\n",
    "        \n",
    "        print(f\"CSV  loaded. Count: {csv_df.count()}\")\n",
    "\n",
    "        # --- 단계 2: Delta Lake에 저장 (Spark SQL 사용) ---\n",
    "        csv_df.write.format(\"delta\").mode(\"overwrite\").save(delta_s3_path)\n",
    "        print(f\"Successfully wrote data to Delta table {delta_s3_path}.\")\n",
    "        \n",
    "        # --- 단계 3: S3 CSV 파일 이동 ---\n",
    "        print(f\"Archiving S3 CSV file...\")\n",
    "        move_s3_file(s3_config, csv_source_key, csv_archive_key)       \n",
    "\n",
    "        # --- 단계 4: PostgreSQL에 삭제 후 삽입 ---   \n",
    "        pg_table_name = pg_config['table']\n",
    "\n",
    "        print(f\"Inserting data  into PostgreSQL table: {pg_table_name}\")\n",
    "         # 데이터를 가공할 Spark SQL 쿼리\n",
    "        insert_sql = f\"\"\"\n",
    "                        select  `전철역명` station_name\n",
    "                           ,`전철역코드` station_cd\n",
    "                           ,`전철명명_영문`  station_ename\n",
    "                           ,`전철명명_중문`  station_cname\n",
    "                           ,`전철명명_일문`  station_jname\n",
    "                           ,`외부코드`  station_cd_external\n",
    "                           , case when substring(`호선`,1,1) = '0' then substring(`호선`,2,length(`호선`)-1) \n",
    "                                  when `호선` = '공항철도' then '공항철도 1호선' \n",
    "                                  when `호선` = '수인분당선' then '수인선'\n",
    "                                  when `호선` = '신분당선' then '분당선'\n",
    "                                  when `호선` = '우이신설경전철' then '우이신설선' \n",
    "                                  else `호선` end   line_no\n",
    "                        from delta.`{delta_s3_path}`\n",
    "                    \"\"\"      \n",
    "        print(insert_sql) \n",
    "        delta_df = spark.sql(insert_sql)\n",
    "        delta_df.write \\\n",
    "            .jdbc(url=pg_url,\n",
    "                  table=pg_table_name,\n",
    "                  mode=\"overwrite\",\n",
    "                  properties=pg_properties)\n",
    "        print(f\"Successfully wrote data to PostgreSQL table {pg_table_name}.\")\n",
    "\n",
    "        print(\"--- Processing for   completed successfully! ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during processing : {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc16c87a-b886-45aa-a5d3-a1ec1d4aee72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " bucket name is paasup\n",
      "\n",
      "--- Starting processing for subway_info ---\n",
      "Reading CSV from s3a://paasup/datasource/서울교통공사_노선별 지하철역 정보.csv...\n",
      "+----------+--------+-------------+------+--------+-------------+-------------+\n",
      "|전철역코드|전철역명|전철명명_영문|  호선|외부코드|전철명명_중문|전철명명_일문|\n",
      "+----------+--------+-------------+------+--------+-------------+-------------+\n",
      "|      1724|    평택|   Pyeongtaek|01호선|    P165|          平?|   ピョンテク|\n",
      "|      1701|    구로|         Guro|01호선|     141|         九老|         クロ|\n",
      "|      1002|    남영|     Namyeong|01호선|     134|          南?|     ナミョン|\n",
      "|      1706|    안양|       Anyang|01호선|    P147|          安?|     アニヤン|\n",
      "|       159|  동묘앞|      Dongmyo|01호선|     127|           ??| トンミョアプ|\n",
      "+----------+--------+-------------+------+--------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CSV  loaded. Count: 796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/28 09:46:14 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/05/28 09:46:26 WARN AmazonHttpClient: SSL Certificate checking for endpoints has been explicitly disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote data to Delta table s3a://paasup/deltalake/subway_info.\n",
      "Archiving S3 CSV file...\n",
      "Copying S3 object from datasource/서울교통공사_노선별 지하철역 정보.csv to archive/서울교통공사_노선별 지하철역 정보.csv...\n",
      "Deleting source S3 object: datasource/서울교통공사_노선별 지하철역 정보.csv...\n",
      "Successfully moved datasource/서울교통공사_노선별 지하철역 정보.csv to archive/서울교통공사_노선별 지하철역 정보.csv.\n",
      "Inserting data  into PostgreSQL table: subway_info\n",
      "\n",
      "                        select  `전철역명` station_name\n",
      "                           ,`전철역코드` station_cd\n",
      "                           ,`전철명명_영문`  station_ename\n",
      "                           ,`전철명명_중문`  station_cname\n",
      "                           ,`전철명명_일문`  station_jname\n",
      "                           ,`외부코드`  station_cd_external\n",
      "                           , case when substring(`호선`,1,1) = '0' then substring(`호선`,2,length(`호선`)-1) \n",
      "                                  when `호선` = '공항철도' then '공항철도 1호선' \n",
      "                                  when `호선` = '수인분당선' then '수인선'\n",
      "                                  when `호선` = '신분당선' then '분당선'\n",
      "                                  when `호선` = '우이신설경전철' then '우이신설선' \n",
      "                                  else `호선` end   line_no\n",
      "                        from delta.`s3a://paasup/deltalake/subway_info`\n",
      "                    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host '172.16.50.29'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host '172.16.50.29'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote data to PostgreSQL table subway_info.\n",
      "--- Processing for   completed successfully! ---\n"
     ]
    }
   ],
   "source": [
    "# --- 메인 실행 블록 ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Pod 내의  .env 파일에서 환경변수 read     \n",
    "    dotenv_path = './config/.env'\n",
    "    load_dotenv(dotenv_path=dotenv_path)    \n",
    "    \n",
    "    # --- APP구성 변수 설정  ---\n",
    "    APP_NAME = 'load_subway_info'\n",
    "    NOTEBOOK_NAME      = \"test\"\n",
    "    NOTEBOOK_NAMESPACE = \"demo01-kf\"    \n",
    "    S3_CONFIG = {\n",
    "        \"access_key\": os.getenv(\"S3_ACCESS_KEY\"),\n",
    "        \"secret_key\": os.getenv(\"S3_SECRET_KEY\"),\n",
    "        \"endpoint\": os.getenv(\"S3_ENDPOINT_URL\"),\n",
    "        \"bucket\":  os.getenv(\"S3_BUCKET_NAME\"),\n",
    "        \"delta_path\": \"deltalake/subway_info\" # S3 버킷 내 Delta target 테이블 경로\n",
    "    }\n",
    "\n",
    "    PG_CONFIG = {\n",
    "        \"host\": os.getenv(\"PG_HOST\"),\n",
    "        \"port\": os.getenv(\"PG_PORT\", \"5432\"), # 기본값 5432 지정\n",
    "        \"dbname\": os.getenv(\"PG_DB\"),\n",
    "        \"user\": os.getenv(\"PG_USER\"),\n",
    "        \"password\": os.getenv(\"PG_PASSWORD\"),\n",
    "        \"table\": \"subway_info\" # PostgreSQL target 테이블 이름\n",
    "    }\n",
    "    # SparkSession 생성\n",
    "    spark = create_spark_session(\n",
    "        app_name=APP_NAME,\n",
    "        notebook_name=NOTEBOOK_NAME,\n",
    "        notebook_namespace=NOTEBOOK_NAMESPACE,\n",
    "        s3_access_key=S3_CONFIG['access_key'],\n",
    "        s3_secret_key=S3_CONFIG['secret_key'],\n",
    "        s3_endpoint=S3_CONFIG['endpoint']\n",
    "        # app_name은 기본값을 사용하므로 전달하지 않아도 됩니다.\n",
    "    )   \n",
    "    \n",
    "    # 데이터 처리 실행\n",
    "    subway_info_overwrite(spark, S3_CONFIG, PG_CONFIG)\n",
    "    \n",
    "\n",
    "    # # SparkSession 종료\n",
    "    # spark.stop()\n",
    "    # print(\"Spark session stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a98649af-4c79-4144-aac0-5f50ae0e85a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
