{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75fbcc82-3c93-421b-8574-ce2be1472ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.functions import lit, substring, col\n",
    "from delta.tables import DeltaTable \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2 \n",
    "import boto3\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e5cfddc-9a69-4a81-8209-36bc0340fc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameters\n",
    "WK_YM = '202502'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90840c35-cf88-44cf-9833-ec587b6c6f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_running_in_jupyter():\n",
    "    try:\n",
    "        # IPython 환경인지 확인\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter Notebook 또는 QtConsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # 터미널에서 실행되는 IPython\n",
    "        else:\n",
    "            return False  # 기타 환경\n",
    "    except NameError:\n",
    "        return False      # IPython이 아님 (일반 Python 인터프리터)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db6d8b3a-c8f3-400a-890b-f152638bccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_config_env():\n",
    "    if is_running_in_jupyter():\n",
    "        # Pod 내의  .env 파일에서 환경변수 read     \n",
    "        dotenv_path = './../../config/.env'\n",
    "        load_dotenv(dotenv_path=dotenv_path)    \n",
    "\n",
    "        # S3_CONFIG, PG_CONFIG 설정 \n",
    "        S3_CONFIG = {\n",
    "            \"access_key\": os.getenv(\"S3_ACCESS_KEY\"),\n",
    "            \"secret_key\": os.getenv(\"S3_SECRET_KEY\"),\n",
    "            \"endpoint\": os.getenv(\"S3_ENDPOINT_URL\"),\n",
    "            \"bucket\":  os.getenv(\"S3_BUCKET_NAME\")\n",
    "        }\n",
    "\n",
    "        PG_CONFIG = {\n",
    "            \"host\": os.getenv(\"PG_HOST\"),\n",
    "            \"port\": os.getenv(\"PG_PORT\", \"5432\"), # 기본값 5432 지정\n",
    "            \"dbname\": os.getenv(\"PG_DB\"),\n",
    "            \"user\": os.getenv(\"PG_USER\"),\n",
    "            \"password\": os.getenv(\"PG_PASSWORD\")\n",
    "        }\n",
    "\n",
    "    return S3_CONFIG, PG_CONFIG \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "572414db-c408-47c2-a9cf-3d662ded77fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session(app_name,  notebook_name, notebook_namespace, s3_access_key, s3_secret_key, s3_endpoint):\n",
    "\n",
    "    conf = SparkConf()\n",
    "    # Spark driver, executor 설정\n",
    "    conf.set(\"spark.submit.deployMode\", \"client\")\n",
    "    conf.set(\"spark.executor.instances\", \"1\")\n",
    "    conf.set(\"spark.executor.memory\", \"1G\")\n",
    "    conf.set(\"spark.driver.memory\", \"1G\")\n",
    "    conf.set(\"spark.executor.cores\", \"1\")\n",
    "    conf.set(\"spark.kubernetes.namespace\", notebook_namespace)\n",
    "    conf.set(\"spark.kubernetes.container.image\", \"paasup/spark:3.5.2-java17-python3.11-2\")\n",
    "    conf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"default-editor\")\n",
    "    conf.set(\"spark.kubernetes.driver.pod.name\", os.environ[\"HOSTNAME\"])\n",
    "    conf.set(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    conf.set(\"spark.driver.host\", notebook_name+ \"-headless.\" + notebook_namespace + \".svc.cluster.local\")\n",
    "    conf.set(\"spark.driver.port\", \"51810\")        \n",
    "    conf.set(\"spark.broadcast.port\", \"51811\")     \n",
    "    conf.set(\"spark.blockManager.port\", \"51812\")\n",
    "\n",
    "    # s3, delta, postgresql 사용 시 필요한 jar 패키지 설정\n",
    "    jar_list = \",\".join([\n",
    "    \"org.apache.hadoop:hadoop-common:3.3.4\",\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "    \"com.amazonaws:aws-java-sdk:1.11.655\", \n",
    "    \"io.delta:delta-spark_2.12:3.3.1\", # Keep if you might use Delta Lake for other operations\n",
    "    \"org.postgresql:postgresql:42.7.2\" # Added for PostgreSQL\n",
    "    ])\n",
    "    conf.set(\"spark.jars.packages\", jar_list)  \n",
    "\n",
    "    # s3 세팅\n",
    "    conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    conf.set('spark.hadoop.fs.s3a.path.style.access', 'true')\n",
    "    conf.set('spark.hadoop.fs.s3a.connection.ssl.enabled', 'true')\n",
    "    conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    conf.set('spark.hadoop.fs.s3a.access.key', s3_access_key)\n",
    "    conf.set('spark.hadoop.fs.s3a.secret.key', s3_secret_key)\n",
    "    conf.set('spark.hadoop.fs.s3a.endpoint', s3_endpoint)\n",
    "    \n",
    "    ### ssl 검증 비활성화\n",
    "    conf.set(\"spark.driver.extraJavaOptions\", \"-Dcom.amazonaws.sdk.disableCertChecking=true\")\n",
    "    conf.set(\"spark.executor.extraJavaOptions\", \"-Dcom.amazonaws.sdk.disableCertChecking=true\")\n",
    "\n",
    "    # deltalake 세팅\n",
    "    conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
    "\n",
    "    # --- SparkSession 빌드 ---\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(conf=conf) \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b9bda6f-67f0-491e-a7fe-eb3dd21345e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_s3_file(s3_config, source_key, dest_key):\n",
    "    \"\"\"\n",
    "    boto3를 사용하여 S3 내에서 파일을 이동(복사 후 삭제)합니다.\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=s3_config['access_key'],\n",
    "        aws_secret_access_key=s3_config['secret_key'],\n",
    "        endpoint_url=s3_config['endpoint'],\n",
    "        verify=False # 자체 서명 인증서 사용 시 필요할 수 있음\n",
    "    )\n",
    "    try:\n",
    "        copy_source = {'Bucket': s3_config['bucket'], 'Key': source_key}\n",
    "        print(f\"Copying S3 object from {source_key} to {dest_key}...\")\n",
    "        s3_client.copy_object(CopySource=copy_source, Bucket=s3_config['bucket'], Key=dest_key)\n",
    "        print(f\"Deleting source S3 object: {source_key}...\")\n",
    "        s3_client.delete_object(Bucket=s3_config['bucket'], Key=source_key)\n",
    "        print(f\"Successfully moved {source_key} to {dest_key}.\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"Error moving S3 file: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during S3 move: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d809b0e-7443-4377-ae62-ae9c50d4a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subway_data_append(spark, s3_config, pg_config):  #pg_config, pg_table, pg_url, pg_properties, delete_sql, insert_sql):\n",
    "    \"\"\"\n",
    "    CSV--> DELTA --> POSTGRESQL 처리하는 메인 파이프라인\n",
    "    \"\"\"\n",
    "    pg_url = f\"jdbc:postgresql://{pg_config['host']}:{pg_config['port']}/{pg_config['dbname']}\"\n",
    "    pg_properties = {\n",
    "        \"user\": pg_config['user'],\n",
    "        \"password\": pg_config['password'],\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "    print(f\" bucket name is {s3_config['bucket']}\")\n",
    "    csv_source_key = f\"datasource/CARD_SUBWAY_MONTH_{WK_YM}.csv\"\n",
    "    csv_archive_key = f\"archive/CARD_SUBWAY_MONTH_{WK_YM}.csv\"\n",
    "    csv_s3_path = f\"s3a://{s3_config['bucket']}/{csv_source_key}\"\n",
    "    delta_s3_path = f\"s3a://{s3_config['bucket']}/deltalake/subway_passengers\"\n",
    "    pg_table_name = \"subway_passengers\"\n",
    "    \n",
    "    try:    \n",
    "        print(f\"\\n--- Starting processing for {pg_table_name} during '{WK_YM}' ---\")\n",
    "\n",
    "        # --- 단계 1: S3 CSV 읽기 ---\n",
    "        print(f\"Reading CSV from {csv_s3_path}...\")\n",
    "        # CSV 스키마나 옵션(overwrite, inferSchema...)은 소스 파일에 맞게 조정\n",
    "        csv_df = spark.read.option(\"header\", \"true\").option(\"overwriteSchema\", \"true\").csv(csv_s3_path)\n",
    "        # '사용일자' 컬럼이 YYYYMMDD 형식이라고 가정\n",
    "        # 필요시 yyyymm 컬럼을 추가하거나 기존 컬럼을 확인\n",
    "        # csv_df = csv_df.withColumn(\"operation_month\", lit(yyyymm)) # 필요시 추가\n",
    "        csv_df.createOrReplaceTempView(\"subway_data\")\n",
    "        print(f\"CSV for {WK_YM} loaded. Count: {csv_df.count()}\")\n",
    "        csv_df.show(5)\n",
    "\n",
    "        # --- 단계 2: Delta Lake에 삭제 후 삽입 (Spark SQL 사용) ---\n",
    "        print(f\"Deleting old data for {WK_YM} from Delta table: {delta_s3_path}\")\n",
    "        \n",
    "        if DeltaTable.isDeltaTable(spark, delta_s3_path):\n",
    "            # '사용일자' 컬럼 형식에 맞게 WHERE 조건 수정\n",
    "            print(f\" Delta Table {delta_s3_path} 있음\") \n",
    "            delete_delta_sql = f\"delete from delta.`{delta_s3_path}` where `사용일자`  like '{WK_YM}'||'%' \"\n",
    "            delta_mode = \"append\"\n",
    "            try:\n",
    "                spark.sql(delete_delta_sql)\n",
    "                print(f\"Deletion from Delta complete. Inserting new data for {WK_YM}...\")\n",
    "            except Exception as e:\n",
    "                print(f\"Skip on error occurred during Delta DELETE, even though table exists: {e}\")\n",
    "                raise # 예상치 못한 오류는 다시 발생시킴         \n",
    "        else:\n",
    "            print(f\" Delta Table {delta_s3_path} 없음\") \n",
    "            delta_mode = \"overwrite\"\n",
    "        \n",
    "        # DataFrame API로 쓰기 (SQL INSERT INTO도 가능하지만, DF API가 S3 쓰기에 더 일반적)\n",
    "        csv_df.write.format(\"delta\").mode(delta_mode).save(delta_s3_path)\n",
    "        print(f\"Successfully wrote data to Delta table {delta_s3_path}.\")\n",
    "        \n",
    "        # --- 단계 3: S3 CSV 파일 이동 ---\n",
    "        print(f\"Archiving S3 CSV file...\")\n",
    "        move_s3_file(s3_config, csv_source_key, csv_archive_key)\n",
    "        \n",
    "\n",
    "        # --- 단계 4: PostgreSQL에 삭제 후 삽입 ---   \n",
    "        print(f\"Deleting old data for {WK_YM} from PostgreSQL table: {pg_table_name}\")        \n",
    "        conn = None\n",
    "        cursor = None\n",
    "        conn = psycopg2.connect (\n",
    "                    host=pg_config['host'],\n",
    "                    port=pg_config['port'],\n",
    "                    dbname=pg_config['dbname'],\n",
    "                    user=pg_config['user'],\n",
    "                    password=pg_config['password'] )\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        delete_sql = f\" delete from {pg_table_name} where use_date like  '{WK_YM}'||'%' \"\n",
    "        print(f\"Executing DELETE query: {delete_sql}\")\n",
    "        cursor.execute(delete_sql)\n",
    "        deleted_rows = cursor.rowcount\n",
    "        if cursor: cursor.close()\n",
    "        if conn: \n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "        print(f\"Successfully executed DELETE query. {deleted_rows} rows deleted.\")    \n",
    "\n",
    "        print(f\"Inserting data for {WK_YM} into PostgreSQL table: {pg_table_name}\")\n",
    "         # 데이터를 가공할 Spark SQL 쿼리\n",
    "        insert_sql = f\"\"\"\n",
    "                        select  `사용일자` use_date\n",
    "                           ,`노선명` line_no\n",
    "                           ,`역명`  station_name\n",
    "                           ,cast(`승차총승객수` as int) pass_in\n",
    "                           ,cast(`하차총승객수` as int) pass_out\n",
    "                           ,substring(`등록일자`,1,8) reg_date\n",
    "                        from delta.`{delta_s3_path}`\n",
    "                        where `사용일자` like '{WK_YM}'||'%'\n",
    "                    \"\"\"      \n",
    "        print(insert_sql) \n",
    "        delta_df = spark.sql(insert_sql)\n",
    "        delta_df.write \\\n",
    "            .jdbc(url=pg_url,\n",
    "                  table=pg_table_name,\n",
    "                  mode=\"append\",\n",
    "                  properties=pg_properties)\n",
    "        print(f\"Successfully wrote data to PostgreSQL table {pg_table_name}.\")\n",
    "\n",
    "        print(f\"--- Processing for {WK_YM} completed successfully! ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during processing {WK_YM}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc16c87a-b886-45aa-a5d3-a1ec1d4aee72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Access Key: 3JHV0NGHJP378DJ4CXJ484GFFTIQQ2CW5CQFWJBM64K0QN2LSTIKKBJP1\n",
      " bucket name is paasup\n",
      "\n",
      "--- Starting processing for subway_passengers during '202504' ---\n",
      "Reading CSV from s3a://paasup/datasource/CARD_SUBWAY_MONTH_202504.csv...\n",
      "CSV for 202504 loaded. Count: 18515\n",
      "+--------+------+-----------------------+------------+------------+--------+\n",
      "|사용일자|노선명|                   역명|승차총승객수|하차총승객수|등록일자|\n",
      "+--------+------+-----------------------+------------+------------+--------+\n",
      "|20250401| 2호선|                   시청|       31730|       30459|20250404|\n",
      "|20250401| 2호선|             을지로입구|       55089|       57583|20250404|\n",
      "|20250401| 2호선|              을지로3가|       27974|       28001|20250404|\n",
      "|20250401| 2호선|              을지로4가|       17026|       17012|20250404|\n",
      "|20250401| 2호선|동대문역사문화공원(DDP)|       15742|       18349|20250404|\n",
      "+--------+------+-----------------------+------------+------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Deleting old data for 202504 from Delta table: s3a://paasup/deltalake/subway_passengers\n",
      " Delta Table s3a://paasup/deltalake/subway_passengers 있음\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/02 09:18:11 WARN AmazonHttpClient: SSL Certificate checking for endpoints has been explicitly disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deletion from Delta complete. Inserting new data for 202504...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/02 09:18:13 WARN AmazonHttpClient: SSL Certificate checking for endpoints has been explicitly disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote data to Delta table s3a://paasup/deltalake/subway_passengers.\n",
      "Archiving S3 CSV file...\n",
      "Copying S3 object from datasource/CARD_SUBWAY_MONTH_202504.csv to archive/CARD_SUBWAY_MONTH_202504.csv...\n",
      "Deleting source S3 object: datasource/CARD_SUBWAY_MONTH_202504.csv...\n",
      "Successfully moved datasource/CARD_SUBWAY_MONTH_202504.csv to archive/CARD_SUBWAY_MONTH_202504.csv.\n",
      "Deleting old data for 202504 from PostgreSQL table: subway_passengers\n",
      "Executing DELETE query:  delete from subway_passengers where use_date like  '202504'||'%' \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host '172.16.50.29'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host '172.16.50.29'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed DELETE query. 18515 rows deleted.\n",
      "Inserting data for 202504 into PostgreSQL table: subway_passengers\n",
      "\n",
      "                        select  `사용일자` use_date\n",
      "                           ,`노선명` line_no\n",
      "                           ,`역명`  station_name\n",
      "                           ,cast(`승차총승객수` as int) pass_in\n",
      "                           ,cast(`하차총승객수` as int) pass_out\n",
      "                           ,substring(`등록일자`,1,8) reg_date\n",
      "                        from delta.`s3a://paasup/deltalake/subway_passengers`\n",
      "                        where `사용일자` like '202504'||'%'\n",
      "                    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote data to PostgreSQL table subway_passengers.\n",
      "--- Processing for 202504 completed successfully! ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# --- 메인 실행 블록 ---\n",
    "if __name__ == \"__main__\":\n",
    "    # input parameters\n",
    "    WK_YM = '202504'\n",
    "    \n",
    "    \n",
    "    # --- APP구성 변수 설정  ---\n",
    "    APP_NAME = 'load_subway_passengers'\n",
    "    NOTEBOOK_NAME      = \"test\"\n",
    "    NOTEBOOK_NAMESPACE = \"demo01-kf\"    \n",
    "\n",
    "    # 환경변수에서 S3_CONFIG, PG_CONFIG setting\n",
    "    S3_CONFIG = {}\n",
    "    PG_CONFIG = {}\n",
    "    S3_CONFIG, PG_CONFIG = set_config_env()\n",
    "    \n",
    "    # SparkSession 생성\n",
    "    spark = create_spark_session(\n",
    "        app_name=APP_NAME,\n",
    "        notebook_name=NOTEBOOK_NAME,\n",
    "        notebook_namespace=NOTEBOOK_NAMESPACE,\n",
    "        s3_access_key=S3_CONFIG['access_key'],\n",
    "        s3_secret_key=S3_CONFIG['secret_key'],\n",
    "        s3_endpoint=S3_CONFIG['endpoint']\n",
    "        # app_name은 기본값을 사용하므로 전달하지 않아도 됩니다.\n",
    "    )   \n",
    "    \n",
    "    # 데이터 처리 실행\n",
    "    subway_data_append(spark, S3_CONFIG, PG_CONFIG)\n",
    "    \n",
    "\n",
    "    # # SparkSession 종료\n",
    "    # spark.stop()\n",
    "    # print(\"Spark session stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7014193d-1aed-469e-9f7c-986e8a8d550f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3JHV0NGHJP378DJ4CXJ484GFFTIQQ2CW5CQFWJBM64K0QN2LSTIKKBJP1\n"
     ]
    }
   ],
   "source": [
    "# Pod 내의  .env 파일에서 환경변수 read     \n",
    "dotenv_path = './../../config/.env'\n",
    "load_dotenv(dotenv_path=dotenv_path)    \n",
    "print(os.getenv(\"S3_ACCESS_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a98649af-4c79-4144-aac0-5f50ae0e85a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
